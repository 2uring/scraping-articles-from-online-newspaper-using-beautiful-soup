# -*- coding: utf-8 -*-
"""web_scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nZPzhJTOsyoYDOqR-Nxggb3apWvSCi8N
"""

import requests
from bs4 import BeautifulSoup

result = requests.get("https://www.thehindu.com/archive/")
result.status_code #200 if successfull

src = result.content
soup = BeautifulSoup(src,'html.parser')
links = soup.find_all("a", href=True)

scrapLink = []

for link in links:
  x = link['href']
  scrapLink.append(x)

# print(scrapLink)

l2010 = [link for link in scrapLink if "web/2010" in link]
# print(l2010)

monthRes = []    
for l in l2010:
    status = 0
    while status != 200:
      req = requests.get(l)
      status = req.status_code
      if status == 200:
        monthRes.append(req)

monthSrc = []
for res in monthRes:
    monthSrc.append(res.content)

monthSoup = []
for src in monthSrc:
    monthSoup.append(BeautifulSoup(src,'html.parser'))

monthLinks = []
for s in monthSoup:
    monthLinks.append(s.find_all("a", href=True))

publishLinks = []
for month in monthLinks:
    for l in month:
      x = l['href']
      if "2010" in x:
        publishLinks.append(x)

# print(publishLinks)

# publishLinks = publishLinks[]
# print(publishLinks)

publishRes = []
print("in")
i = 0
for link in publishLinks:
  i+= 1
  print(i)  #print statement to see progress till 365
  status = 0
  while status != 200:        #loop until request is successfull
    req = requests.get(link)
    status = req.status_code
    if status == 200:
      publishRes.append(req)

print("out")
# publishRes.append(requests.get(publishLinks))

print("No. of responses = ",len(publishRes))
print(1)


publishSrc = []
for res in publishRes:
    publishSrc.append(res.content)
print("No. of sources = ",len(publishSrc))
print(2)

publishSoup = []
for src in publishSrc:    # one soup for each of 365 publications
    publishSoup.append(BeautifulSoup(src, 'html.parser'))
print("No. of published soups: ",len(publishSoup))

import re 

allArticles = []   # this stores links to all the articles publisehd over 10 years.
scrapLink = []


for soup in publishSoup:
  
  
  archiveUL = []
  ulList = soup.find_all("ul")
  for ul in ulList:
    if ul.has_attr('class'):
      if ul.attrs['class'][0] == 'archive-list':
        archiveUL.append(ul)


  for ul in archiveUL:
    links = ul.find_all("a", href=True)
  # print(len(links))
    for link in links:   
      if not (link.has_attr('class')):  ##to remove current trending articles which have a separate class on each published page
        x = link.attrs['href'] 
        scrapLink.append(x)


for link in scrapLink:

  if re.search("article", str(link)) and re.search(".ece", str(link)):
      allArticles.append(link)

print(len(scrapLink))

print(len(allArticles))  # total no. of articles published over the year 2010.

#input and searching each document for the input word



import re
from IPython.display import clear_output

count = 0
myWord = str(input("Enter the word you want to search for: "))
myWord = myWord.lower()

i = 0



#can use multithreading here to reduce query time


for articleLink in allArticles: 
  # i += 1
  # print(i)  
  status = 0
  while status != 200:
    result = requests.get(articleLink)
    status = result.status_code
  
  src = result.content
  soup = BeautifulSoup(src, 'html.parser')
  divs = soup.find_all("div")

  para = []
  for d in divs:
    if d.has_attr('id'):
      if d.attrs['id'].startswith("content-body"):
        para.append(d.find_all("p"))


  myPara = ""
  for p in para:
    myPara += str(p)
  myPara = myPara.lower()

  if re.search(myWord, myPara):
    count += 1
  clear_output()
  print("Found {} articles with a mention of {}".format(count, myWord))


print("{} articles with the word {} were publised in 2010".format(count, myWord))

